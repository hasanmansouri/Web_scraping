{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6d61b46-5fbd-4e13-a0bd-33f5b7dfa3f3",
   "metadata": {},
   "source": [
    "# Web Scraping Project: Extracting Company Data  \n",
    "\n",
    "\n",
    "This project extracts company names and details from a website using **Selenium** and saves them in CSV files.  \n",
    "\n",
    "## Tools & Methods I used\n",
    "- **Selenium**: Automates browser actions  \n",
    "- **Pandas**: Stores data in CSV format  \n",
    "- **Chrome WebDriver**: Controls the browser  \n",
    "- **Explicit Waits**: Ensures elements load before interacting  \n",
    "- **Try-Except Blocks**: Handles errors  \n",
    "\n",
    "\n",
    "I divided the project into **Two steps:**\n",
    "\n",
    "\n",
    "## Step 1: Extracting Company Names  \n",
    "\n",
    "## Step 2: Extracting Company Details  \n",
    "\n",
    "## Issues I faced and fixed:\n",
    "\n",
    "- **Page reset after clicking next:** After clicking on \"previous page\" button it comes back to the first page.  \n",
    "- **Missing data:** Used `try-except` to prevent errors  \n",
    "- **Search issues:** Cleared and re-entered search terms  \n",
    "\n",
    "## Final Output  \n",
    "- **Company names** in a text file  \n",
    "- **Company details** in CSV files  \n",
    "- **Fully automated process**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58adba0e-283d-47d6-ac16-84b53b785c6c",
   "metadata": {},
   "source": [
    "### First part of the project: To extract all the comanies' names and save it to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7d5c0d-a75c-40c3-a897-1559616e9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b30e5b-70dc-4f31-b169-97119039bf79",
   "metadata": {},
   "source": [
    "Here we've put all parts of the code in one cell because the program has to do all these steps at once.\n",
    "\n",
    "\n",
    "- Start the webdriver and opens the chrome to go to the target url\n",
    "- Waits for the page to load compeletely\n",
    "- We made a list where the names of the companies will be saved\n",
    "- The program uses the CSS selector that we've provided to find all elements containing company names on the current page.\n",
    "- For each company element found, the program extracts the company name (company_element.text) and appends it to the company_names list.\n",
    "- After extracting names from the current page, the program looks for the \"Next Page\" button.\n",
    "- If the button is found and clickable, it scrolls to the button and clicks it, then waits for the next page to load\n",
    "- And at the end it breaks (stops) when there are no more pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f09f3-76e8-49a0-ad45-3f296c798cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using webdriver:\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://example.com/path-to-data\")\n",
    "\n",
    "#Waiting for the page to load:\n",
    "time.sleep(5)  \n",
    "\n",
    "\n",
    "#Extracting companies' names:\n",
    "company_names = []\n",
    "\n",
    "while True:\n",
    "    #Finding companies' names on the page:\n",
    "    company_elements = driver.find_elements(By.CSS_SELECTOR, \"p.MuiTypography-root.MuiTypography-body1.css-8uynmr\")\n",
    "    for company_element in company_elements:\n",
    "        company_names.append(company_element.text)\n",
    "\n",
    "    try:\n",
    "        #Next page button:\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Seguente')]\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "        next_button.click()\n",
    "        \n",
    "        #Waiting for the next page to load:\n",
    "        time.sleep(5)  \n",
    "\n",
    "    except Exception:\n",
    "        print(\"No more pages or error while clicking 'Next Page'. Stopping.\")\n",
    "        #Break if there are no more pages:\n",
    "        break  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dab142-a1d2-48cd-9627-ee997c8ca6a2",
   "metadata": {},
   "source": [
    "Then we save the companies names from the list we made to a CSV file to use it for the second part of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3f35d-3e07-47f2-a5e2-a1cc90434b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the companies' names in a CSV file:\n",
    "with open(\"company_names.csv\", \"w\") as f:\n",
    "    for name in company_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f35f8-bcd4-4cb1-8ee5-6e04a38804f9",
   "metadata": {},
   "source": [
    "Now we move on to the **Second part** of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e5c2c-a8df-4e1d-be89-93ceca5049f9",
   "metadata": {},
   "source": [
    "### Second part of the project: \n",
    "### To use the company_names.csv and extract the required info of the companies one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9cdd7-54d4-4d46-a22e-39d424a1742e",
   "metadata": {},
   "source": [
    "We import the libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0180d63-25ea-47ea-815f-5763be18aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have imported these libraries in the previous part but I just wrote them to remind that we need them also here:\n",
    "#from selenium import webdriver\n",
    "#from selenium.webdriver.common.by import By\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "#from selenium.webdriver.support import expected_conditions as EC\n",
    "#import time\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8d5d8-6b51-45e4-8d52-a79d917410af",
   "metadata": {},
   "source": [
    "During the project we faced some phone numbers that are ended with slash mark \"/\" and the bot couldn't extract them well so here we defined a function to prevent this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de630def-e6ef-4b01-8cb7-54934fcb646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_phone_number(phone):\n",
    "    return phone.replace('/', '').strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c72b7ff-d302-47c2-aa14-263e20373b09",
   "metadata": {},
   "source": [
    "And another issue that was occured was found out if the program crashes, there will be no saved CSV file and the extracted data before the crash would be lost. So here we defined another function to actively save the info of each company as we go to the next one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4880e3-a4c7-47c4-9e11-9c209938864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename):\n",
    "    pd.DataFrame(data).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ba806-4fce-4c5f-b9aa-779ba195c876",
   "metadata": {},
   "source": [
    "We start the Chrome WebDriver again, using `ChromeOptions` for customization. And we open the URL.\n",
    "\n",
    "This step loads the webpage to begin the scraping process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381223e-fc86-4c0c-9813-b82c23d1b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://example.com/path-to-data\"\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d4620-f524-4590-a5df-38853f4831a8",
   "metadata": {},
   "source": [
    "To reduce the risk of code crash and data loss I divided the `company_names.csv` to 3 smaller CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850edaa3-37bb-403f-840e-9250b74172a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_files = [\"company_names.csv\", \"company_names2.csv\", \"company_names3.csv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d605d-dc3c-4ce3-b72c-e2635ebf735c",
   "metadata": {},
   "source": [
    "Again I put all parts of the code at once because it has to do all the steps together.\n",
    "\n",
    "- Loop through company files: Goes to each CSV file containing the companies' names starting with `company_names.csv` to scrape data for each company.\n",
    "\n",
    "\n",
    " - For each company name:\n",
    "   - Wait for the search bar to load on the webpage.\n",
    "   - Enter the company name in the search bar and hit enter.\n",
    "   - Wait for the company result to appear on the page.\n",
    "   - Click on the first search result to navigate to the company details page.\n",
    "   - Extract the company's address, 2 phone numbers, and website. The addresses are written in 3 lines, first line was for the street name and number, the second line was the name of the city, and third line was for the CAP(5-digit number) and \"Italia\". There were 2 phone numbers for each company which had 2 different CSS selectors so I extracted them seperately and saved them in 2 different columns (as the costumer wanted them) \n",
    "   - If data is missing, by default `\"not found\"` will be saved.\n",
    "  \n",
    "- After extracting data for one company, reload the page to prepare for the next search.\n",
    "\n",
    "- After processing all companies in the current file, save the data into a CSV file with a name based on the original input file.\n",
    "\n",
    "- Continue this process for each csv file in `company_file` list until all files have been processed.\n",
    "\n",
    "- End of scraping: Once all files have been processed, close the browser and print a success message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a4cdb-e6ec-47ba-bb17-5ecae488de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for company_file in company_files:\n",
    "    df = pd.read_csv(company_file)\n",
    "    company_names = df[\"Company Name\"].tolist()\n",
    "    #Store the extracted details:\n",
    "    data = []  \n",
    "\n",
    "    for company in company_names:\n",
    "        try:\n",
    "            #Loading search bar:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"input[placeholder='Cerca espositore per nome']\"))\n",
    "            )\n",
    "\n",
    "            #Search for the company name:\n",
    "            search_bar = driver.find_element(By.CSS_SELECTOR, \"input[placeholder='Cerca espositore per nome']\")\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", search_bar)\n",
    "            time.sleep(1)\n",
    "            search_bar.click()\n",
    "            #Select all text at once because if not, the search bar starts searching the remaining characters:\n",
    "            search_bar.send_keys(Keys.CONTROL + \"a\")\n",
    "            #Delete the selected text:\n",
    "            search_bar.send_keys(Keys.BACKSPACE)\n",
    "            #Type the next company name:\n",
    "            search_bar.send_keys(company)\n",
    "            time.sleep(2)\n",
    "            search_bar.send_keys(Keys.RETURN) \n",
    "\n",
    "            #Waiting for the search result:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, f\"//p[contains(text(), '{company}')]\"))\n",
    "            )\n",
    "\n",
    "            #Click on the first search result:\n",
    "            search_result = driver.find_element(By.XPATH, f\"//p[contains(text(), '{company}')]\")\n",
    "            search_result.click()\n",
    "\n",
    "            #Wait for the company page to load:\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"MuiStack-root\")))\n",
    "\n",
    "            #Extract Address:\n",
    "            try:\n",
    "                address_lines = driver.find_elements(By.XPATH, \"//p[contains(@class, 'MuiTypography-body2 css-jpwjwb')]\")\n",
    "                address = \", \".join([line.text.strip() for line in address_lines[:3]])  #Extract first 3 lines\n",
    "            except:\n",
    "                address = \"Address not found\"\n",
    "\n",
    "            #Extract Phone 1:\n",
    "            try:\n",
    "                phone1_element = driver.find_element(By.XPATH, \"//p[contains(@class, 'MuiTypography-body2')]/span\")\n",
    "                phone1 = clean_phone_number(phone1_element.text)\n",
    "            except:\n",
    "                phone1 = \"Phone 1 not found\"\n",
    "\n",
    "            #Extract Phone 2:\n",
    "            try:\n",
    "                phone2_elements = driver.find_elements(By.XPATH, \"//p[contains(@class, 'MuiTypography-body2')]\")\n",
    "                phone2 = None\n",
    "                for element in phone2_elements:\n",
    "                    text = element.text.strip()\n",
    "                    if text.startswith(\"+\") and \"Tel:\" not in text:\n",
    "                        phone2 = text\n",
    "                        break\n",
    "                phone2 = phone2 if phone2 else \"Phone 2 not found\"\n",
    "            except:\n",
    "                phone2 = \"Phone 2 not found\"\n",
    "\n",
    "            #Extract Website:\n",
    "            try:\n",
    "                website = driver.find_element(By.XPATH, \"//span[contains(@class, 'MuiTypography-body2-bold') and contains(text(), 'www')]\").text\n",
    "            except:\n",
    "                website = \"Website not found\"\n",
    "\n",
    "            #Store the data in 5 columns:\n",
    "            data.append({\n",
    "                \"Company Name\": company,\n",
    "                \"Phone 1\": phone1,\n",
    "                \"Phone 2\": phone2,\n",
    "                \"Address\": address,\n",
    "                \"Website\": website\n",
    "            })\n",
    "\n",
    "            #Reload the page to avoid errors:\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "\n",
    "\n",
    "        #print the error (if there was any) and the company name where it occured\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {company}: {e}\")\n",
    "\n",
    "    #Saving the extracted data to CSV:\n",
    "    output_filename = company_file.replace(\"company_names\", \"company_details\")\n",
    "    save_to_csv(data, output_filename)\n",
    "\n",
    "    \n",
    "    data.clear()\n",
    "\n",
    "driver.quit()\n",
    "print(\"Scraping completed for all files!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7824c15-90d3-47d8-bb97-2cb5db6dfb85",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "For the last part of the project I had to clean the extracted and saved data.\n",
    "\n",
    "I had to:\n",
    "- Merge the 3 result CSV files that each were extracted from one of the 3 company names files\n",
    "- Check if there are any duplicated data and remove them\n",
    "- Compare the \"Company name\" column of the result file to the original company name CSV file and check if there is any missing company and if there is save them to a `.txt` file.\n",
    "- Add the missing companies' data to the result file\n",
    "- Extract the CAP code (the 5-digit code) from the Address column, make another column and add the CAPs for each company\n",
    "- And finally group the result file by CAPs starting with same digit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf931f-6ecd-4f56-8f11-08fc7ca04691",
   "metadata": {},
   "source": [
    "### Merging the 3 result files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b3136-1235-4a6b-a9c1-612978bd6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is no need to import Pandas as we did it before\n",
    "\n",
    "#Loading the three result CSV files:\n",
    "df1 = pd.read_csv(\"company_details.csv\")\n",
    "df2 = pd.read_csv(\"company_details2.csv\")\n",
    "df3 = pd.read_csv(\"company_details3.csv\")\n",
    "\n",
    "#Merging:\n",
    "merged_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "#Saving:\n",
    "merged_df.to_csv(\"merged_company_details.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63af59-4f79-4ecb-99e8-4b1b56c6ec1a",
   "metadata": {},
   "source": [
    "### Removing duplicate data (if there is any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662210f6-0fee-4f2c-a012-bf9e76cb792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"merged_company_details.csv\")\n",
    "\n",
    "#Removing duplicates from the \"Company Name\" column:\n",
    "merged_df_cleaned = merged_df.drop_duplicates(subset=\"Company Name\", keep='first')\n",
    "\n",
    "#Saving:\n",
    "merged_df_cleaned.to_csv(\"merged_company_details_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63f208-44c7-4601-893d-a6234d471423",
   "metadata": {},
   "source": [
    "### Finding and handling missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f62576-c684-4575-8a6d-b17032909be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the merged file:\n",
    "merged_df = pd.read_csv(\"merged_company_details.csv\")\n",
    "\n",
    "#Reading the original company names from the text file:\n",
    "with open(\"company_names.txt\", \"r\", encoding=\"latin1\") as file:\n",
    "    company_names = [line.strip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "#Getting the company names and make a list of them:\n",
    "scraped_companies = merged_df[\"Company Name\"].tolist()\n",
    "\n",
    "#Finding missing company names:\n",
    "missing_companies = [name for name in company_names if name not in scraped_companies]\n",
    "\n",
    "#Saving missing company names to a text file:\n",
    "with open(\"missing_companies.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for company in missing_companies:\n",
    "        file.write(company + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab208ccf-774d-4c17-b135-5e1a5dd19616",
   "metadata": {},
   "source": [
    "### Adding missing values \n",
    "We did it manually because there were just 4 of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55617da5-475f-45f0-be02-02e3d125599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the merged company details CSVL\n",
    "merged_csv = \"merged_company_details_cleaned.csv\"\n",
    "df = pd.read_csv(merged_csv)\n",
    "\n",
    "#List of missing companies\n",
    "missing_companies = [\n",
    "    {\n",
    "        \"Company Name\": \"company 1\",\n",
    "        \"Address\": \"address 1\",\n",
    "        \"Phone 1\": \"###########\",\n",
    "        \"Phone 2\": \"##########\",\n",
    "        \"Website\": \"www.#####.com\"\n",
    "    },\n",
    "    {\n",
    "        \"Company Name\": \"company 2\",\n",
    "        \"Address\": \"address 2\",\n",
    "        \"Phone 1\": \"##########\",\n",
    "        \"Phone 2\": \"##########\",\n",
    "        \"Website\": \"www.#######.it\"\n",
    "    },\n",
    "    {\n",
    "        \"Company Name\": \"company 3\",\n",
    "        \"Address\": \"address 3\",\n",
    "        \"Phone 1\": \"###########\",\n",
    "        \"Phone 2\": \"###########\",\n",
    "        \"Website\": \"www.#######.it\"\n",
    "    },\n",
    "    {\n",
    "        \"Company Name\": \"company 4\",\n",
    "        \"Address\": \"address 4\",\n",
    "        \"Phone 1\": \"############\",\n",
    "        \"Phone 2\": \"############\",\n",
    "        \"Website\": \"www.########.com\"\n",
    "    }\n",
    "]\n",
    "\n",
    "#Convert list to a DataFrame:\n",
    "missing_df = pd.DataFrame(missing_companies)\n",
    "\n",
    "#Appending missing companies to the main DataFrame:\n",
    "df = pd.concat([df, missing_df], ignore_index=True)\n",
    "\n",
    "#Saving:\n",
    "df.to_csv(\"merged_company_details_cleaned_updated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657172c6-34d8-4191-84dd-14bc5d7d8256",
   "metadata": {},
   "source": [
    "### Extracting the CAP from the addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205cfb0-9375-4c2a-81e9-d6aef15e7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Loading the merged and cleaned CSV:\n",
    "merged_df_cleaned = pd.read_csv(\"merged_company_details_cleaned.csv\")\n",
    "\n",
    "#Function to extract CAP from the address:\n",
    "def extract_cap(address):\n",
    "    #regex (AI did this part)\n",
    "    match = re.search(r'\\b\\d{5}\\b', address)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "#Applying the CAP extraction function to the 'Address' column:\n",
    "merged_df_cleaned['CAP'] = merged_df_cleaned['Address'].apply(extract_cap)\n",
    "\n",
    "#Group by the first digit of the CAP:\n",
    "merged_df_cleaned['CAP Group'] = merged_df_cleaned['CAP'].apply(lambda x: x[0] if x else None)\n",
    "\n",
    "#Groupping the data by the new 'CAP Group' column:\n",
    "grouped_df = merged_df_cleaned.groupby('CAP Group')\n",
    "\n",
    "#Saving to CSV:\n",
    "grouped_df_list = [group for _, group in grouped_df]\n",
    "final_df = pd.concat(grouped_df_list, ignore_index=True)\n",
    "\n",
    "#Save the final grouped data to a CSV file:\n",
    "final_df.to_csv(\"final_grouped_company_details.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc54315-bc16-45d7-8dcf-a7b31c34d16d",
   "metadata": {},
   "source": [
    "### The End :]\n",
    "\n",
    "Done by `Hassan Mansouri` \n",
    "Linkedin: www.linkedin.com/in/hassan-mansourii/  \n",
    "Github: www.github.com/hasanmansouri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba88b04-0a67-43ba-80e4-da677f28c657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
